{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender by ALS\n",
    "\n",
    "With Collaborative filtering, we make predictions (filtering) about the interests of a user by collecting preferences or taste information from many users (collaborating). The underlying assumption is that if a user A has the same opinion as a user B on an issue, A is more likely to have B’s opinion on a different issue x than to have the opinion on x of a user-chosen randomly.\n",
    "\n",
    "At first, people rate different items (like videos, images, games). Then, the system makes predictions about a user’s rating for an item not rated yet. The new predictions are built upon the existing ratings of other users with similar ratings with the active user.\n",
    "\n",
    "Matrix factorization: is a class of collaborative filtering algorithms used in recommender systems. Matrix factorization algorithms work by decomposing the user-item interaction matrix into the product of two lower dimensionality rectangular matrices.\n",
    "\n",
    "Alternating least square(ALS) matrix factorization: The idea is basically to take a large (or potentially huge) matrix and factor it into some smaller representation of the original matrix through alternating least squares. We end up with two or more lower dimensional matrices whose product equals the original one.ALS comes inbuilt in Apache Spark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.9:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.6</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "\n",
    "ROOT_DIR = \"./\"\n",
    "DATA_DIR = path.join(ROOT_DIR, 'data')\n",
    "MODEL_DIR = path.join(ROOT_DIR, 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5460385L"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Utilities definition\n",
    "\"\"\"\n",
    "\n",
    "import hashlib\n",
    "\n",
    "class Utils:\n",
    "    \n",
    "    '''\n",
    "    TODO Transform the ID to unique integer\n",
    "    '''\n",
    "    @staticmethod\n",
    "    def hashToInt(s):\n",
    "        return int(hashlib.sha1(s).hexdigest(), 16) % (10 ** 8)\n",
    "\n",
    "#Test\n",
    "Utils.hashToInt('A141HP4LYPWMSR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Read ratings data from file to RDD\n",
    "Split each line into 4 parts separated by the commas \n",
    "\"\"\"\n",
    "\n",
    "path = os.path.join(DATA_DIR, 'ratings')\n",
    "data = sc.textFile(path).map(lambda l: l.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyspark\n",
    "\n",
    "# data.persist(pyspark.StorageLevel.MEMORY_AND_DISK)\n",
    "# data.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create rdd named 'ratings' which is:\n",
    "    ` A set of 'Ratings' object\n",
    "    ` Combination of 3 features: product id as Int, user id as Int, rating score as float \n",
    "\"\"\"\n",
    "\n",
    "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n",
    "\n",
    "ratings = data.map(lambda (i, p, u, r): Rating(Utils.hashToInt(p), Utils.hashToInt(u), float(r)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Store userId in Int and movieIn in Int data \n",
    "    corresponding to their origin for later usage\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "users = data.map(lambda (i, p,u,r): (Utils.hashToInt(u), u)).toDF()\n",
    "movies = data.map(lambda (i, p, u, r): (Utils.hashToInt(p), p)).toDF()\n",
    "\n",
    "user_path = os.path.join(DATA_DIR, 'users')\n",
    "movie_path = os.path.join(DATA_DIR,'movies')\n",
    "\n",
    "users.write.csv(user_path, mode='overwrite', compression='gzip')\n",
    "movies.write.csv(movie_path, mode='overwrite', compression='gzip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Split dataset to trainset, testset\n",
    "    ` Trainset: 80% random records\n",
    "    ` Testset: 20% remained\n",
    "    \n",
    "X_train: movie and user of each review from Trainset\n",
    "X_test: movie and user of each review from Testset\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "(train_data, test_data) = ratings.randomSplit([0.8, 0.2])\n",
    "\n",
    "X_train = train_data.map(lambda (p, u, r): (int(p), int(u)))\n",
    "\n",
    "X_test = test_data.map(lambda (p, u, r): (int(p), int(u)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6328853"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1582831"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Buiding Recommeder using matrix factorization method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Recommendation model using ALS\n",
    "    ` rank = 10\n",
    "    ` number of iteration = 10 \n",
    "\n",
    "===> Need to try other hyper parameters to find the best model \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "rank = 10\n",
    "numIterations = 10\n",
    "\n",
    "model = ALS.train(train_data, rank, numIterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Evaluation \n",
    "\n",
    "In this part we evaluate the recommendation model on both train dataset and test dataset. \n",
    "The RMSE lost function will be used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import RegressionMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error = 0.349035992892\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on training data\n",
    "\n",
    "predictions = model.predictAll(X_train).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "\n",
    "labelsAndPreds = train_data.map(lambda (p, u, r): ((p,u),r)) \\\n",
    "                                .join(predictions).map(lambda (k, v): v)\n",
    "\n",
    "\n",
    "metrics = RegressionMetrics(labelsAndPreds)\n",
    "\n",
    "rmse = metrics.rootMeanSquaredError\n",
    "\n",
    "print(\"Root Mean Squared Error = \" + str(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error = 1.12854318275\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Why the rmse on train and test data are same??????????'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model on test data\n",
    "\n",
    "test_preds = model.predictAll(X_test).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "\n",
    "test_labelsAndPreds = test_data.map(lambda (p, u, r): ((p,u),r)) \\\n",
    "                                .join(test_preds).map(lambda (k, v): v) \\\n",
    "\n",
    "test_metrics = RegressionMetrics(test_labelsAndPreds)\n",
    "\n",
    "test_rmse = test_metrics.rootMeanSquaredError\n",
    "\n",
    "print(\"Root Mean Squared Error = \" + str(test_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save(sc, \"./model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc.stop()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "When to use RDDs?\n",
    "Consider these scenarios or common use cases for using RDDs when:\n",
    "– you want low-level transformation and actions and control on your dataset;\n",
    "– your data is unstructured, such as media streams or streams of text;\n",
    "– you want to manipulate your data with functional programming constructs than domain\n",
    "specific expressions;\n",
    "– you don’t care about imposing a schema, such as columnar format, while processing or accessing data attributes by name or column; and\n",
    "– you can forgo some optimization and performance benefits available with DataFrames and Datasets for structured and semi-structured data.\n",
    "When should I use DataFrames or Datasets?\n",
    "– If you want rich semantics, high-level abstractions, and domain specific APIs, use DataFrame or Dataset.\n",
    "– If your processing demands high-level expressions, filters, maps, aggregation, averages, sum, SQL queries, columnar access and use of lambda functions on semi-structured data, use DataFrame or Dataset.\n",
    "– If you want higher degree of type-safety at compile time, want typed JVM objects, take advantage of Catalyst optimization, and benefit from Tungsten’s efficient code generation, use Dataset.\n",
    "– If you want unification and simplification of APIs across Spark Libraries, use DataFrame or Dataset.\n",
    "– If you are a R user, use DataFrames.\n",
    "– If you are a Python user, use DataFrames and resort back to RDDs if you need more control.r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
